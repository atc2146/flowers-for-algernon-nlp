{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP on Flowers for Algernon\n",
    "\n",
    "\n",
    "Information\n",
    "<ul>\n",
    "    <li>https://en.wikipedia.org/wiki/Flowers_for_Algernon</li>\n",
    "</ul>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "import textstat\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually created list of valid words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corresponds to the progress report x\n",
    "valid_words_manual = dict()\n",
    "\n",
    "valid_words_manual[1] = []\n",
    "valid_words_manual[2] = ['inkblot']\n",
    "valid_words_manual[3] = ['hollers']\n",
    "valid_words_manual[4] = ['apperception']\n",
    "valid_words_manual[5] = []\n",
    "valid_words_manual[6] = []\n",
    "valid_words_manual[7] = []\n",
    "valid_words_manual[8] = []\n",
    "valid_words_manual[9]= ['backfired', 'commas', 'photostated', 'ditched', 'halloran', 'scarves', 'twitches', 'whiter', 'valentines', 'valentine', 'dimples', 'bunches', 'teases', 'scribbles', 'caretakers', 'wf', 'ddf', 'sf', 'obj', 'inkblots']\n",
    "valid_words_manual[10] = ['rubs','cringes', 'clumping', 'sta', 'bakers', 'kneads', 'smooths', 'snorts', 'rechecked', 'hallston', 'neurosurgeons', 'excites', 'dostoevski', 'flaubert', 'teardrops', 'pecking', 'untwists', 'shrieks', 'cowers', 'overwhelms', 'spanked', 'dirties']\n",
    "valid_words_manual[11] = ['toyed', 'highlighted', 'swirls', 'frightens', 'outstretched', 'rubs', 'amuses', 'spanked', 'éclairs', 'éclairs', 'undercharging', 'natured', 'deliveries', 'wavered', 'shhhh', 'mazes', 'undercharging', 'etymologies', 'blacklisting', 'ii', 'wessey', 'ii', 'trobriand', 'debussy', 'browned', 'dopes', 'cheeked', 'howells', 'swishes', 'writhes', 'forgets', 'cheeked']\n",
    "valid_words_manual[12] = ['cupids', 'blurts', 'shrieks', 'glares', 'shrieks', 'fades', 'dorni', 'wheelchair', 'smartest', 'wheelchair', 'francine', 'trainees', 'variants', 'boulean', 'dawned', 'widening', 'splintering', 'simmering', 'zagging', 'outcropping', 'crawlers']\n",
    "valid_words_manual[13] = ['strato', 'uhm', 'vibrations', 'vibrations', 'twitches', 'ushers', 'glares', 'pursing', 'ushering', 'psychosubstantiation', 'pats', 'cringes', 'cringes', 'shivers', 'dirties', 'shortwave', 'encephalo', 'smartest', 'flushes', 'pats', 'squeezes', 'donkeys', 'forgets', 'frightens', 'pats', 'whimpers', 'quarrelled', 'phenylketonuria', 'injections', 'revitalized', 'tanida', 'psychopathology', 'tanida', 'propounded', 'outshines', 'dumbfounded', 'tanida', 'petrology', 'banach', 'manifolds', 'motorized', 'mugged', 'jung', 'phoniness', 'euphemisms', 'handshakers', 'nodders', 'smilers', 'standees', 'zellerman', 'worfel', 'resenting', 'gawking', 'scurrying', 'minted', 'scampered', 'experimentalists', 'xiv', 'washbasins', 'midtown', 'shoebox']\n",
    "valid_words_manual[14] = ['legwork', 'penetrates', 'sinks', 'surrounds', 'baghdad', 'taping', 'mealtimes', 'gateleg', 'pretzels', 'shambles', 'oozing', 'overstuffed', 'underthings', 'flinging', 'burbled', 'burglaries', 'tresses', 'uptilted', 'dilettantes', 'dilettantes', 'dolled', 'stardust', 'coffins', 'junkmobiles', 'cliché', 'mazes', 'curtsied', 'gordons', 'phinney', 'meiner', 'cheeked', 'barbershops', 'apologizing', 'wakens', 'shaves', 'stardust', 'stardust', 'cooed', 'yawned', 'weirdest', 'weirdest', 'dimwits', 'immersing', 'westerns', 'whistles', 'catcalls', 'mazel', 'tov', 'dishware', 'cowered', 'rowdier', 'abusing', 'naïveté', 'cheeked', 'crosstown', 'outstretched', 'toyed', 'brassière', 'underthings', 'landsdoff', 'photoeffect', 'befriended', 'stardust', 'recorders', 'undermines']\n",
    "valid_words_manual[15] = ['disposing', 'pieced', 'injections', 'psycho']\n",
    "valid_words_manual[16] = ['morons', 'cuddling', 'jangled', 'thursdays', 'lautrec', 'retardates', 'wheelchair', 'cuddling', 'mutes', 'laundries', 'bakeries', 'untidies', 'stardust', 'waltzing', 'sunbathing', 'cabarets', 'cheee', 'cheeeee', 'urinating', 'defecating', 'cheeee', 'cheeeee', 'cheeeee', 'stardust', 'panicking', 'fades', 'hyram', 'hyram', 'raynor', 'raynor', 'raynor', 'raynor', 'retardates', 'raynor', 'raynors', 'raynor', 'wiggles', 'shhhh', 'shhhh', 'spluttered', 'likeable', 'gordons', 'effacing', 'slurred', 'novocaine', 'regressed', 'syndromes', 'rechecked', 'hallston', 'undrapes', 'radioisotopes', 'convolutions', 'fissures', 'couldn', 'shhhh', 'gashed', 'fingermarks', 'coolies', 'lampshades', 'fussed', 'spanked', 'duncecap', 'morons', 'raskin', 'lampshades', 'platitudes', 'passageways', 'burdening', 'barked']\n",
    "valid_words_manual[17] = ['abusing', 'encompassing', 'multipetaled', 'rainbows', 'multipetaled', 'retyped', 'mazes', 'fugues', 'humoring', 'humoring', 'kiddie', 'unclosing', 'reclaiming', 'über', 'psychische', 'ganzheit', 'mooney', 'windmills', 'sorcerers', 'windowshade', 'nov']\n",
    "\n",
    "more_valid_words = []\n",
    "for k,v in valid_words_manual.items():\n",
    "    for word in v:\n",
    "        more_valid_words.append(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_words = nltk.corpus.words.words()\n",
    "words_words_lower = set([i.lower() for i in words_words])\n",
    "\n",
    "brown_words = nltk.corpus.brown.words()\n",
    "brown_words_lower = set([i.lower() for i in brown_words])\n",
    "\n",
    "valid_words = ['', 've', 'll','charlie', 'harold', 'nemur', 'alice', 'kinnian', 'algernon', 'guarino', 'beekman', \n",
    "               'welberg', 'hymie', 'rahajamati', 'jayson', 'strauss', 'burt', 'selden', 'fay', 'lillman',\n",
    "              'herman', 'donner', 'frank,', 'reilly','joe', 'carp', 'gimpy', 'fanny', 'birden',\n",
    "              'hilda', 'minnie', 'meyer', 'klaus', 'norma', 'matt', 'gordon', 'krueger',\n",
    "              ]\n",
    "\n",
    "valid_words.extend(more_valid_words)\n",
    "\n",
    "all_lower_words = set.union(*[words_words_lower, brown_words_lower, valid_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234377\n",
      "49815\n",
      "294\n",
      "261822\n"
     ]
    }
   ],
   "source": [
    "print(len(words_words_lower))\n",
    "print(len(brown_words_lower))\n",
    "print(len(set(valid_words)))\n",
    "print(len(all_lower_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.word_tokenize(\"I couldn't stay at hadn't doesn't Dr. Neuman the party.\".lower())\n",
    "\n",
    "# print('don\\'t' in all_lower_words)\n",
    "# print('wasn\\'t' in all_lower_words)\n",
    "# print('shouldn\\'t' in all_lower_words)\n",
    "# print('rite' in words_words_lower)\n",
    "# print('faled' in words_words_lower)\n",
    "# print('smartest' in words_words_lower)\n",
    "# print(\"prof\" in words_words_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Archived Files/flowers-for-algernon-2005.txt', encoding='utf-8') as reader:\n",
    "    data = reader.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all chapters are in the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the names of the chapters in the book; some chapter names are truncated\n",
    "chapters = ['Progris riport 1', 'Progris riport 2', '3d progris riport',\n",
    "            'Progris riport 4', 'Progris riport 5', 'Progris riport 6',\n",
    "            'PROGRESS REPORT 7', 'PROGRESS REPORT 8', 'PROGRESS REPORT 9',\n",
    "            'PROGRESS REPORT 10', 'PROGRESS REPORT 11', 'PROGRESS REPORT 12',\n",
    "            'PROGRESS REPORT 13', 'PROGRESS REPORT 14', 'PROGRESS REPORT 15',\n",
    "            'PROGRESS REPORT 16', 'PROGRESS REPORT 17']\n",
    "chapters = [i.lower() for i in chapters]\n",
    "\n",
    "# Note this is not 0-indexed (nor should it be, it's a dict)\n",
    "chapters_dict = dict()\n",
    "for chapter_index in range(0, len(chapters)):\n",
    "    chapters_dict[chapter_index+1]= chapters[chapter_index]\n",
    "\n",
    "# Check if all chapters are in the book\n",
    "for chapter in chapters:\n",
    "    current_count = (data.lower()).count(chapter)\n",
    "    assert current_count == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place each chapter's text (and relevant info) into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_chapters = []\n",
    "\n",
    "for chapter in chapters_dict:\n",
    "    \n",
    "    start_chapter_index = (data.lower()).find(chapters_dict[chapter])\n",
    "    \n",
    "    if(chapter==len(chapters_dict)):\n",
    "        end_chapter_index = len(data)\n",
    "    else:\n",
    "        end_chapter_index = (data.lower()).find(chapters_dict[chapter+1])\n",
    "    \n",
    "    chapter_number = chapter    \n",
    "    chapter_name = chapters_dict[chapter]\n",
    "    chapter_text = data[start_chapter_index:end_chapter_index]\n",
    "    \n",
    "    list_of_chapters.append((chapter_number, chapter_name, chapter_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a custom tokenization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make this a class and use inheritence/polymorphism\n",
    "\n",
    "pattern = r'''[.|,|\\-|\\'|—|_|*|?|\\\\\"|:|;|^|$|#|@|!|(|)|+|=|%|&|(\\d)+]+'''\n",
    "def custom_token(some_list):\n",
    "    \"\"\"\n",
    "    Further tokenization of the text\n",
    "    \"\"\"\n",
    "    new_list = []\n",
    "    correct_abbreviations = ['dr.', 'mr.', 'mrs.', 'miss.', 'prof.']\n",
    "    for word in some_list:\n",
    "        match = re.search(pattern, word)\n",
    "        if(match) and (word not in correct_abbreviations):\n",
    "            split_list = re.sub(pattern, \" \", word).strip().split(' ')\n",
    "            new_list.extend(split_list)\n",
    "        else:\n",
    "            new_list.append(word)\n",
    "    #remove empty tokens\n",
    "    new_list = list(filter(None, new_list))\n",
    "    \n",
    "    return new_list\n",
    "\n",
    "def custom_token_2(some_token):\n",
    "    \"\"\"\n",
    "    Further tokenization of the a single token\n",
    "    \"\"\"\n",
    "    correct_abbreviations = ['dr.', 'mr.', 'mrs.', 'miss.', 'prof.']\n",
    "\n",
    "    match = re.search(pattern, some_token)\n",
    "    if(match) and (some_token not in correct_abbreviations):\n",
    "        split_list = re.sub(pattern, \" \", some_token).strip().split(' ')\n",
    "        split_list = list(filter(None, split_list))\n",
    "    else:\n",
    "        split_list = [some_token]\n",
    "    \n",
    "    return split_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the words in each chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_list_of_chapters = []\n",
    "\n",
    "for chapter in list_of_chapters:\n",
    "    chapter_number = chapter[0]\n",
    "    chapter_name = chapter[1]\n",
    "    chapter_tokens_original = nltk.word_tokenize(chapter[2])\n",
    "    chapter_tokens = nltk.word_tokenize(chapter[2].lower())\n",
    "    chapter_tokens = custom_token(chapter_tokens)\n",
    "    \n",
    "    tokenized_list_of_chapters.append((chapter_number, chapter_name, chapter_tokens, chapter_tokens_original))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get sentence surrounding a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(token_index: int, chapter_tokens_original: list, offset: int=5)-> str:\n",
    "    # get 11 words ahead\n",
    "    if(token_index-offset<0):\n",
    "        start_index=0\n",
    "    else:\n",
    "        start_index = token_index-offset+1\n",
    "        \n",
    "    return \" \".join(chapter_tokens_original[start_index:token_index+5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the surrounding sentence to each word; Tokenize all words (each row is a token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_tokenized = []\n",
    "offset = 5\n",
    "word_pos = 0\n",
    "\n",
    "for chapter_info in tokenized_list_of_chapters:\n",
    "    chapter_num = chapter_info[0]\n",
    "    chapter_name = chapter_info[1]\n",
    "    chapter_tokens = chapter_info[2]\n",
    "    chapter_tokens_original = chapter_info[3]\n",
    "    \n",
    "    for token_index in range(0, len(chapter_tokens_original)):\n",
    "        if(token_index > len(chapter_tokens_original) + offset):\n",
    "            break\n",
    "\n",
    "        sentence = get_sentence(token_index, chapter_tokens_original, offset)\n",
    "\n",
    "        all_words_tokenized.append([chapter_num, chapter_name, word_pos, chapter_tokens_original[token_index], sentence])\n",
    "        \n",
    "        word_pos += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the words again (using the custom function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list_tokenized = list()\n",
    "\n",
    "for token_info in all_words_tokenized:\n",
    "    \n",
    "    chapter_number = token_info[0]\n",
    "    chapter_name = token_info[1]\n",
    "    word_pos = token_info[2]\n",
    "    original_token = token_info[3]\n",
    "    sentence = token_info[4]\n",
    "    \n",
    "    custom_tokens = custom_token_2(original_token.lower())\n",
    "    \n",
    "    for new_token in custom_tokens:\n",
    "        final_list_tokenized.append((chapter_number, chapter_name, word_pos, new_token, sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the final list of all words tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort based on word position\n",
    "# sorted(final_list_tokenized, key=lambda x: x[2])\n",
    "\n",
    "final_list_tokenized_with_filters = list()\n",
    "\n",
    "for token_info in final_list_tokenized:\n",
    "    word = token_info[3]\n",
    "    if word not in all_lower_words:\n",
    "        word_misspelled = True\n",
    "    else:\n",
    "        word_misspelled = False\n",
    "    \n",
    "    final_list_tokenized_with_filters.append((*token_info, word_misspelled))\n",
    "    \n",
    "final_list_tokenized_with_filters.sort(key=lambda x: x[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chapter Number</th>\n",
       "      <th>Chapter Name</th>\n",
       "      <th>Word Position</th>\n",
       "      <th>Token</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Misspelled</th>\n",
       "      <th>Word Position in Chapter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>progris riport 1</td>\n",
       "      <td>1</td>\n",
       "      <td>progris</td>\n",
       "      <td>Progris riport 1 martch 3</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>progris riport 1</td>\n",
       "      <td>2</td>\n",
       "      <td>riport</td>\n",
       "      <td>Progris riport 1 martch 3 Dr</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>progris riport 1</td>\n",
       "      <td>3</td>\n",
       "      <td>martch</td>\n",
       "      <td>Progris riport 1 martch 3 Dr Strauss says</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>progris riport 1</td>\n",
       "      <td>4</td>\n",
       "      <td>dr</td>\n",
       "      <td>riport 1 martch 3 Dr Strauss says I shoud</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>progris riport 1</td>\n",
       "      <td>5</td>\n",
       "      <td>strauss</td>\n",
       "      <td>1 martch 3 Dr Strauss says I shoud rite</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89615</th>\n",
       "      <td>17</td>\n",
       "      <td>progress report 17</td>\n",
       "      <td>89616</td>\n",
       "      <td>grave</td>\n",
       "      <td>some flowrs on Algernons grave in the bak yard</td>\n",
       "      <td>False</td>\n",
       "      <td>9757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89616</th>\n",
       "      <td>17</td>\n",
       "      <td>progress report 17</td>\n",
       "      <td>89617</td>\n",
       "      <td>in</td>\n",
       "      <td>flowrs on Algernons grave in the bak yard .</td>\n",
       "      <td>False</td>\n",
       "      <td>9758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89617</th>\n",
       "      <td>17</td>\n",
       "      <td>progress report 17</td>\n",
       "      <td>89618</td>\n",
       "      <td>the</td>\n",
       "      <td>on Algernons grave in the bak yard .</td>\n",
       "      <td>False</td>\n",
       "      <td>9759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89618</th>\n",
       "      <td>17</td>\n",
       "      <td>progress report 17</td>\n",
       "      <td>89619</td>\n",
       "      <td>bak</td>\n",
       "      <td>Algernons grave in the bak yard .</td>\n",
       "      <td>True</td>\n",
       "      <td>9760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89619</th>\n",
       "      <td>17</td>\n",
       "      <td>progress report 17</td>\n",
       "      <td>89620</td>\n",
       "      <td>yard</td>\n",
       "      <td>grave in the bak yard .</td>\n",
       "      <td>False</td>\n",
       "      <td>9761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89620 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Chapter Number        Chapter Name  Word Position    Token  \\\n",
       "0                   1    progris riport 1              1  progris   \n",
       "1                   1    progris riport 1              2   riport   \n",
       "2                   1    progris riport 1              3   martch   \n",
       "3                   1    progris riport 1              4       dr   \n",
       "4                   1    progris riport 1              5  strauss   \n",
       "...               ...                 ...            ...      ...   \n",
       "89615              17  progress report 17          89616    grave   \n",
       "89616              17  progress report 17          89617       in   \n",
       "89617              17  progress report 17          89618      the   \n",
       "89618              17  progress report 17          89619      bak   \n",
       "89619              17  progress report 17          89620     yard   \n",
       "\n",
       "                                             Sentence  Misspelled  \\\n",
       "0                           Progris riport 1 martch 3        True   \n",
       "1                        Progris riport 1 martch 3 Dr        True   \n",
       "2           Progris riport 1 martch 3 Dr Strauss says        True   \n",
       "3           riport 1 martch 3 Dr Strauss says I shoud        True   \n",
       "4             1 martch 3 Dr Strauss says I shoud rite       False   \n",
       "...                                               ...         ...   \n",
       "89615  some flowrs on Algernons grave in the bak yard       False   \n",
       "89616     flowrs on Algernons grave in the bak yard .       False   \n",
       "89617            on Algernons grave in the bak yard .       False   \n",
       "89618               Algernons grave in the bak yard .        True   \n",
       "89619                         grave in the bak yard .       False   \n",
       "\n",
       "       Word Position in Chapter  \n",
       "0                             1  \n",
       "1                             2  \n",
       "2                             3  \n",
       "3                             4  \n",
       "4                             5  \n",
       "...                         ...  \n",
       "89615                      9757  \n",
       "89616                      9758  \n",
       "89617                      9759  \n",
       "89618                      9760  \n",
       "89619                      9761  \n",
       "\n",
       "[89620 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = pd.DataFrame(final_list_tokenized_with_filters,\n",
    "                       columns=['Chapter Number', 'Chapter Name', 'Word Position', 'Token', 'Sentence', 'Misspelled'])\n",
    "df_final['Word Position'] = np.arange(1, len(df_final)+1)\n",
    "df_final['Word Position in Chapter'] = df_final.groupby('Chapter Number')['Chapter Number'].rank(axis=1, method='first').astype(int)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_of_words_chapter(chapter_number: int) -> int:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return len(df_final[df_final['Chapter Number']==chapter_number])\n",
    "\n",
    "\n",
    "def get_num_of_misspellings(chapter_number: int) -> int:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return len(df_final[(df_final['Chapter Number']==chapter_number) & (df_final['Misspelled']==True)])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator function to manually create valid words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chapter_generator():\n",
    "    \"\"\"\n",
    "    Generator function that yields chapter number\n",
    "    and mispellings in that chapter\n",
    "    \n",
    "    Created to manually check mispellings\n",
    "    \"\"\"\n",
    "    for chapter_number, chapter_name in chapters_dict.items():\n",
    "        misspellings = list(set(df_final[(df_final['Chapter Number']==chapter_number) & (df_final['Misspelled']==True)]['Token']))\n",
    "        \n",
    "        yield ((chapter_number, chapter_name), misspellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gen = get_chapter_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'progris riport 1')\n",
      "['perfesser', 'yrs', 'tolld', 'membir', 'evrey', 'shoud', 'compushishens', 'importint', 'beekmin', 'collidge', 'dr', 'riport', 'evrything', 'dollers', 'martch', 'yeres', 'lern', 'becaus', 'munth', 'werk', 'donners', 'kinnians', 'mabye', 'shud', 'happins', 'progris', 'anymor', 'brithday', 'retarted']\n"
     ]
    }
   ],
   "source": [
    "x = next(my_gen)\n",
    "print(x[0])\n",
    "print(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do a sentiment analysis on each chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nrc_data():\n",
    "    nrc = 'Data/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt'\n",
    "    count=0\n",
    "    emotion_dict=dict()\n",
    "    with open(nrc,'r') as f:\n",
    "        all_lines = list()\n",
    "        for line in f:\n",
    "            if count < 46:\n",
    "                count+=1\n",
    "                continue\n",
    "            line = line.strip().split('\\t')\n",
    "            if int(line[2]) == 1:\n",
    "                if emotion_dict.get(line[0]):\n",
    "                    emotion_dict[line[0]].append(line[1])\n",
    "                else:\n",
    "                    emotion_dict[line[0]] = [line[1]]\n",
    "    return emotion_dict\n",
    "\n",
    "emotion_dict = get_nrc_data()\n",
    "\n",
    "def emotion_analyzer(text, emotion_dict=emotion_dict):\n",
    "    #Set up the result dictionary\n",
    "    emotions = {x for y in emotion_dict.values() for x in y}\n",
    "    emotion_count = dict()\n",
    "    for emotion in emotions:\n",
    "        emotion_count[emotion] = 0\n",
    "\n",
    "    #Analyze the text and normalize by total number of words\n",
    "    total_words = len(text.split())\n",
    "    for word in text.split():\n",
    "        if emotion_dict.get(word):\n",
    "            for emotion in emotion_dict.get(word):\n",
    "                emotion_count[emotion] += 1/len(text.split())\n",
    "    return emotion_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get chapter level summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chapter_level = []\n",
    "\n",
    "for chapter in list_of_chapters:\n",
    "    chapter_number = chapter[0]\n",
    "    text = str(chapter[2].replace('\\n', ''))\n",
    "    fre = textstat.flesch_reading_ease(text)\n",
    "    fkg = textstat.flesch_kincaid_grade(text)\n",
    "    si = textstat.smog_index(text)\n",
    "    cli = textstat.coleman_liau_index(text)\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "    dcrs = textstat.dale_chall_readability_score(text)    \n",
    "\n",
    "    lwf = textstat.linsear_write_formula(text)\n",
    "    gf = textstat.gunning_fog(text)\n",
    "    ts = textstat.text_standard(text)\n",
    "\n",
    "    sentence_count = textstat.sentence_count(text)\n",
    "    \n",
    "    num_of_words = get_num_of_words_chapter(chapter_number)\n",
    "    misspellings = get_num_of_misspellings(chapter_number)\n",
    "    \n",
    "    data_chapter_level.append((chapter_number, num_of_words, sentence_count, misspellings,\n",
    "                              fre, fkg, si, cli, ari, dcrs, lwf, gf, ts))\n",
    "    \n",
    "df_chapters = pd.DataFrame(data_chapter_level, columns=['Chapter Number', 'Number of Words', 'Number of Sentences', 'Misspelled Words',\n",
    "                                                        'Flesch Reading Ease', 'Flesch Grade', 'Smog Index', \n",
    "                                                        'Coleman Liau Readability', 'Automated Readability Index',\n",
    "                                                        'Dale-Chall Readability', 'Linsear Write Formula',\n",
    "                                                        'Gunning Fog', 'Text Standard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_spell_correct(token: str, misspelled: bool) -> str:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if(misspelled):\n",
    "        return spell.correction(token)\n",
    "    else:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Correction'] = df_final.apply(lambda x: apply_spell_correct(x['Token'], x['Misspelled']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct spellings\n",
    "list_of_chapters_text = []\n",
    "for chapter_number, chapter_name in chapters_dict.items():\n",
    "    chapter_text = \" \".join(list(df_final[(df_final['Chapter Number']==chapter_number)]['Correction']))\n",
    "    list_of_chapters_text.append((chapter_number, chapter_name, chapter_text))\n",
    "\n",
    "\n",
    "list_to_df_emotions = []\n",
    "for chapter_info in list_of_chapters_text:\n",
    "    list_to_df_emotions.append(emotion_analyzer(chapter_info[2]))\n",
    "    \n",
    "df_emotions = pd.DataFrame(list_to_df_emotions)\n",
    "# add anger, surprise\n",
    "df_emotions['all_pos']=(df_emotions['trust']+df_emotions['positive']+ df_emotions['joy']+ df_emotions['anticipation'])\n",
    "df_emotions['all_neg']=(df_emotions['fear']+df_emotions['negative']+ df_emotions['disgust']+ df_emotions['sadness'])\n",
    "df_emotions['net']=(df_emotions['all_pos']-df_emotions['all_neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chapters = pd.concat([df_chapters, df_emotions], axis=1, sort=False)\n",
    "df_chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('Data/Flowers for Algernon - All word tokens.csv')\n",
    "df_chapters.to_csv('Data/Flowers for Algernon - Chapter summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .generate\n",
    "# generate random text using a trigram model\n",
    "# https://stackoverflow.com/questions/1150144/generating-random-sentences-from-custom-text-in-pythons-nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
