{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP on Flowers for Algernon\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import datetime\n",
    "import csv \n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "import textstat\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually created list of valid words\n",
    "Read a text file with a list of (additional) valid words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_words(file_name):\n",
    "    \"\"\"\n",
    "    Returns a list of words by reading\n",
    "    a text file.  \n",
    "    \n",
    "    Each word should be on a new row in the text\n",
    "    file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The file name of the text file to read.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of the words.  \n",
    "    \"\"\"\n",
    "    words = []\n",
    "    \n",
    "    with open(file_name, 'r') as fp:\n",
    "    # Read each row of the file\n",
    "        reader = csv.reader(fp)\n",
    "        for row in reader:\n",
    "            words.extend(row)    \n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The valid-words'manual.txt contains words that are valid.\n",
    "more_valid_words = return_words('../Data/valid-words-manual.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of valid (lowercase) words\n",
    "From the following:\n",
    "* Brown corpus\n",
    "* Words corpus\n",
    "* The valid words before\n",
    "* And another manually created list (Nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_words = nltk.corpus.words.words()\n",
    "words_words_lower = set([i.lower() for i in words_words])\n",
    "\n",
    "brown_words = nltk.corpus.brown.words()\n",
    "brown_words_lower = set([i.lower() for i in brown_words])\n",
    "\n",
    "# Other valid words - Nouns\n",
    "valid_words = ['', 've', 'll','charlie', 'harold', 'nemur', 'alice', 'kinnian', 'algernon', 'guarino', 'beekman', \n",
    "               'welberg', 'hymie', 'rahajamati', 'jayson', 'strauss', 'burt', 'selden', 'fay', 'lillman',\n",
    "              'herman', 'donner', 'frank,', 'reilly','joe', 'carp', 'gimpy', 'fanny', 'birden',\n",
    "              'hilda', 'minnie', 'meyer', 'klaus', 'norma', 'matt', 'gordon', 'krueger',\n",
    "              ]\n",
    "\n",
    "valid_words.extend(more_valid_words)\n",
    "\n",
    "all_lower_words = set.union(*[words_words_lower, brown_words_lower, valid_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234377\n",
      "49815\n",
      "294\n",
      "261822\n"
     ]
    }
   ],
   "source": [
    "print(len(words_words_lower))\n",
    "print(len(brown_words_lower))\n",
    "print(len(set(valid_words)))\n",
    "print(len(all_lower_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read text of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../Archived Files/flowers-for-algernon-2005.txt', encoding='utf-8') as reader:\n",
    "    data = reader.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all chapters are in the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the names of the chapters in the book; some chapter names are truncated\n",
    "chapters = ['Progris riport 1', 'Progris riport 2', '3d progris riport',\n",
    "            'Progris riport 4', 'Progris riport 5', 'Progris riport 6',\n",
    "            'PROGRESS REPORT 7', 'PROGRESS REPORT 8', 'PROGRESS REPORT 9',\n",
    "            'PROGRESS REPORT 10', 'PROGRESS REPORT 11', 'PROGRESS REPORT 12',\n",
    "            'PROGRESS REPORT 13', 'PROGRESS REPORT 14', 'PROGRESS REPORT 15',\n",
    "            'PROGRESS REPORT 16', 'PROGRESS REPORT 17']\n",
    "chapters = [i.lower() for i in chapters]\n",
    "\n",
    "# Note this is not 0-indexed (nor should it be, it's a dict)\n",
    "chapters_dict = dict()\n",
    "for chapter_index in range(0, len(chapters)):\n",
    "    chapters_dict[chapter_index+1]= chapters[chapter_index]\n",
    "\n",
    "# Check if all chapters are in the book\n",
    "for chapter in chapters:\n",
    "    current_count = (data.lower()).count(chapter)\n",
    "    assert current_count == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place each chapter's text (and relevant info) into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_chapters = []\n",
    "\n",
    "for chapter in chapters_dict:\n",
    "    \n",
    "    start_chapter_index = (data.lower()).find(chapters_dict[chapter])\n",
    "    \n",
    "    if(chapter==len(chapters_dict)):\n",
    "        end_chapter_index = len(data)\n",
    "    else:\n",
    "        end_chapter_index = (data.lower()).find(chapters_dict[chapter+1])\n",
    "    \n",
    "    chapter_number = chapter    \n",
    "    chapter_name = chapters_dict[chapter]\n",
    "    chapter_text = data[start_chapter_index:end_chapter_index]\n",
    "    \n",
    "    list_of_chapters.append((chapter_number, chapter_name, chapter_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a custom tokenization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make this a class and use inheritence/polymorphism\n",
    "\n",
    "pattern = r'''[.|,|\\-|\\'|â€”|_|*|?|\\\\\"|:|;|^|$|#|@|!|(|)|+|=|%|&|(\\d)+]+'''\n",
    "def custom_token(some_list):\n",
    "    \"\"\"\n",
    "    Further tokenization of the text\n",
    "    \"\"\"\n",
    "    new_list = []\n",
    "    correct_abbreviations = ['dr.', 'mr.', 'mrs.', 'miss.', 'prof.']\n",
    "    for word in some_list:\n",
    "        match = re.search(pattern, word)\n",
    "        if(match) and (word not in correct_abbreviations):\n",
    "            split_list = re.sub(pattern, \" \", word).strip().split(' ')\n",
    "            new_list.extend(split_list)\n",
    "        else:\n",
    "            new_list.append(word)\n",
    "    #remove empty tokens\n",
    "    new_list = list(filter(None, new_list))\n",
    "    \n",
    "    return new_list\n",
    "\n",
    "def custom_token_2(some_token):\n",
    "    \"\"\"\n",
    "    Further tokenization of the a single token\n",
    "    \"\"\"\n",
    "    correct_abbreviations = ['dr.', 'mr.', 'mrs.', 'miss.', 'prof.']\n",
    "\n",
    "    match = re.search(pattern, some_token)\n",
    "    if(match) and (some_token not in correct_abbreviations):\n",
    "        split_list = re.sub(pattern, \" \", some_token).strip().split(' ')\n",
    "        split_list = list(filter(None, split_list))\n",
    "    else:\n",
    "        split_list = [some_token]\n",
    "    \n",
    "    return split_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the words in each chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_list_of_chapters = []\n",
    "\n",
    "for chapter in list_of_chapters:\n",
    "    chapter_number = chapter[0]\n",
    "    chapter_name = chapter[1]\n",
    "    chapter_tokens_original = nltk.word_tokenize(chapter[2])\n",
    "    chapter_tokens = nltk.word_tokenize(chapter[2].lower())\n",
    "    chapter_tokens = custom_token(chapter_tokens)\n",
    "    \n",
    "    tokenized_list_of_chapters.append((chapter_number, chapter_name, chapter_tokens, chapter_tokens_original))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get sentence surrounding a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(token_index: int, chapter_tokens_original: list, offset: int=5) -> str:\n",
    "    \"\"\"\n",
    "    Returns the The surrounding words \n",
    "    before and after the token.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    token_index : int\n",
    "        The index of the token.\n",
    "\n",
    "    chapter_tokens_original : list\n",
    "        The tokens in the chapter.\n",
    "\n",
    "    offset : int, default=5\n",
    "        The number of words to include before and after the token_index.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The surrounding words before and after the token. \n",
    "    \"\"\"\n",
    "    # get 11 words ahead\n",
    "    if (token_index - offset < 0):\n",
    "        start_index = 0\n",
    "    else:\n",
    "        start_index = token_index - offset + 1\n",
    "        \n",
    "    return \" \".join(chapter_tokens_original[start_index:token_index+5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the surrounding sentence to each word; Tokenize all words (each row is a token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_tokenized = []\n",
    "offset = 5\n",
    "word_pos = 0\n",
    "\n",
    "for chapter_info in tokenized_list_of_chapters:\n",
    "    chapter_num = chapter_info[0]\n",
    "    chapter_name = chapter_info[1]\n",
    "    chapter_tokens = chapter_info[2]\n",
    "    chapter_tokens_original = chapter_info[3]\n",
    "    \n",
    "    for token_index in range(0, len(chapter_tokens_original)):\n",
    "        if(token_index > len(chapter_tokens_original) + offset):\n",
    "            break\n",
    "\n",
    "        sentence = get_sentence(token_index, chapter_tokens_original, offset)\n",
    "\n",
    "        all_words_tokenized.append([chapter_num, chapter_name, word_pos, chapter_tokens_original[token_index], sentence])\n",
    "        \n",
    "        word_pos += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the words again (using the custom function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list_tokenized = list()\n",
    "\n",
    "for token_info in all_words_tokenized:\n",
    "    \n",
    "    chapter_number = token_info[0]\n",
    "    chapter_name = token_info[1]\n",
    "    word_pos = token_info[2]\n",
    "    original_token = token_info[3]\n",
    "    sentence = token_info[4]\n",
    "    \n",
    "    custom_tokens = custom_token_2(original_token.lower())\n",
    "    \n",
    "    for new_token in custom_tokens:\n",
    "        final_list_tokenized.append((chapter_number, chapter_name, word_pos, new_token, sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the final list of all words tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort based on word position\n",
    "# sorted(final_list_tokenized, key=lambda x: x[2])\n",
    "\n",
    "final_list_tokenized_with_filters = list()\n",
    "\n",
    "for token_info in final_list_tokenized:\n",
    "    word = token_info[3]\n",
    "    if word not in all_lower_words:\n",
    "        word_misspelled = True\n",
    "    else:\n",
    "        word_misspelled = False\n",
    "    \n",
    "    final_list_tokenized_with_filters.append((*token_info, word_misspelled))\n",
    "    \n",
    "final_list_tokenized_with_filters.sort(key=lambda x: x[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(final_list_tokenized_with_filters,\n",
    "                       columns=['Chapter Number', 'Chapter Name', 'Word Position', 'Token', 'Sentence', 'Misspelled'])\n",
    "df_final['Word Position'] = np.arange(1, len(df_final)+1)\n",
    "df_final['Word Position in Chapter'] = df_final.groupby('Chapter Number')['Chapter Number'].rank(axis=1, method='first').astype(int)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_of_words_chapter(chapter_number: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of words in a chapter.\n",
    "    \"\"\"\n",
    "    return len(df_final[df_final['Chapter Number']==chapter_number])\n",
    "\n",
    "\n",
    "def get_num_of_misspellings(chapter_number: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of misspelled words in a chapter.\n",
    "    \"\"\"\n",
    "    return len(df_final[(df_final['Chapter Number']==chapter_number) & (df_final['Misspelled']==True)])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator function to manually create valid words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chapter_generator():\n",
    "    \"\"\"\n",
    "    Generator function that yields chapter number\n",
    "    and mispellings in that chapter\n",
    "    \n",
    "    Created to manually check mispellings\n",
    "    \"\"\"\n",
    "    for chapter_number, chapter_name in chapters_dict.items():\n",
    "        misspellings = list(set(df_final[(df_final['Chapter Number']==chapter_number) & (df_final['Misspelled']==True)]['Token']))\n",
    "        \n",
    "        yield ((chapter_number, chapter_name), misspellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gen = get_chapter_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = next(my_gen)\n",
    "# print(x[0])\n",
    "# print(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do a sentiment analysis on each chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nrc_data():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    nrc = '../Data/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt'\n",
    "    count=0\n",
    "    emotion_dict=dict()\n",
    "    with open(nrc,'r') as f:\n",
    "        all_lines = list()\n",
    "        for line in f:\n",
    "            if count < 46:\n",
    "                count+=1\n",
    "                continue\n",
    "            line = line.strip().split('\\t')\n",
    "            if int(line[2]) == 1:\n",
    "                if emotion_dict.get(line[0]):\n",
    "                    emotion_dict[line[0]].append(line[1])\n",
    "                else:\n",
    "                    emotion_dict[line[0]] = [line[1]]\n",
    "    return emotion_dict\n",
    "\n",
    "emotion_dict = get_nrc_data()\n",
    "\n",
    "def emotion_analyzer(text, emotion_dict=emotion_dict):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Set up the result dictionary\n",
    "    emotions = {x for y in emotion_dict.values() for x in y}\n",
    "    emotion_count = dict()\n",
    "    for emotion in emotions:\n",
    "        emotion_count[emotion] = 0\n",
    "\n",
    "    #Analyze the text and normalize by total number of words\n",
    "    total_words = len(text.split())\n",
    "    for word in text.split():\n",
    "        if emotion_dict.get(word):\n",
    "            for emotion in emotion_dict.get(word):\n",
    "                emotion_count[emotion] += 1/len(text.split())\n",
    "    return emotion_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get chapter level summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chapter_level = []\n",
    "\n",
    "for chapter in list_of_chapters:\n",
    "    chapter_number = chapter[0]\n",
    "    text = str(chapter[2].replace('\\n', ''))\n",
    "    fre = textstat.flesch_reading_ease(text)\n",
    "    fkg = textstat.flesch_kincaid_grade(text)\n",
    "    si = textstat.smog_index(text)\n",
    "    cli = textstat.coleman_liau_index(text)\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "    dcrs = textstat.dale_chall_readability_score(text)    \n",
    "\n",
    "    lwf = textstat.linsear_write_formula(text)\n",
    "    gf = textstat.gunning_fog(text)\n",
    "    ts = textstat.text_standard(text)\n",
    "\n",
    "    sentence_count = textstat.sentence_count(text)\n",
    "    \n",
    "    num_of_words = get_num_of_words_chapter(chapter_number)\n",
    "    misspellings = get_num_of_misspellings(chapter_number)\n",
    "    \n",
    "    data_chapter_level.append((chapter_number, num_of_words, sentence_count, misspellings,\n",
    "                              fre, fkg, si, cli, ari, dcrs, lwf, gf, ts))\n",
    "    \n",
    "df_chapters = pd.DataFrame(data_chapter_level, columns=['Chapter Number', 'Number of Words', 'Number of Sentences', 'Misspelled Words',\n",
    "                                                        'Flesch Reading Ease', 'Flesch Grade', 'Smog Index', \n",
    "                                                        'Coleman Liau Readability', 'Automated Readability Index',\n",
    "                                                        'Dale-Chall Readability', 'Linsear Write Formula',\n",
    "                                                        'Gunning Fog', 'Text Standard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_spell_correct(token: str, misspelled: bool) -> str:\n",
    "    \"\"\"\n",
    "    Returns the best guess at the corrected word given a word.\n",
    "    \"\"\"\n",
    "    if(misspelled):\n",
    "        return spell.correction(token)\n",
    "    else:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Correction'] = df_final.apply(lambda x: apply_spell_correct(x['Token'], x['Misspelled']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct spellings\n",
    "list_of_chapters_text = []\n",
    "for chapter_number, chapter_name in chapters_dict.items():\n",
    "    chapter_text = \" \".join(list(df_final[(df_final['Chapter Number']==chapter_number)]['Correction']))\n",
    "    list_of_chapters_text.append((chapter_number, chapter_name, chapter_text))\n",
    "\n",
    "list_to_df_emotions = []\n",
    "for chapter_info in list_of_chapters_text:\n",
    "    list_to_df_emotions.append(emotion_analyzer(chapter_info[2]))\n",
    "    \n",
    "df_emotions = pd.DataFrame(list_to_df_emotions)\n",
    "# add anger, surprise\n",
    "df_emotions['all_pos']=(df_emotions['trust']+df_emotions['positive']+ df_emotions['joy']+ df_emotions['anticipation'])\n",
    "df_emotions['all_neg']=(df_emotions['fear']+df_emotions['negative']+ df_emotions['disgust']+ df_emotions['sadness'])\n",
    "df_emotions['net']=(df_emotions['all_pos']-df_emotions['all_neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chapters = pd.concat([df_chapters, df_emotions], axis=1, sort=False)\n",
    "df_chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get overall readability of entire book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_book_str = ''\n",
    "\n",
    "for chapter in list_of_chapters:\n",
    "    whole_book_str += str(chapter[2].replace('\\n', ''))\n",
    "\n",
    "print(textstat.text_standard(whole_book_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('Data/Flowers for Algernon - All word tokens.csv')\n",
    "df_chapters.to_csv('Data/Flowers for Algernon - Chapter summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .generate\n",
    "# generate random text using a trigram model\n",
    "# https://stackoverflow.com/questions/1150144/generating-random-sentences-from-custom-text-in-pythons-nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
